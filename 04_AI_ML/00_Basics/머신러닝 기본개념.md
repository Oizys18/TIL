# 머신러닝 기본개념

## 과대적합 Overfitting

- 모델이 엄청 유연해서 학습(훈련) 데이터는 잘 분류하지만, 실제 데이터에선 성능을 발휘하지 못하는 경우
- 일반성이 떨어진다는 뜻
- 너무 복잡한 패턴을 학습하는 경우, 잡음까지 학습하는 경우 등등 발생한다.

- 해결방법
  - 훈련 데이터를 더 많이 모은다. (훈련 데이터의 크기를 늘린다.)
  - 정규화 Regularization (규제,드롭아웃 등)를 통해 적당한 복잡도를 가지는 모델을 찾는다. 
  - 훈련 데이터의 잡음을 줄인다. (오류 수정, 이상치 제거) 

## 과소적합 Underfitting

- 과대적합의 반대, 모델이 너무 단순해서 데이터의 내재된 구조를 학습하지 못할 때 발생
- 해결방법
  - 파라미터가 더 많은 복잡한 모델을 선택
  - 모델의 제약을 줄이기 (규제 하이퍼파라미터 값 줄이기)
  - 조기종료 시점 (overfitting이 되기 전의 시점)까지 충분히 학습

## 일반화 Generalization

- 과대적합, 과소적합을 피하고 테스트 데이터에 대한 높은 성능을 갖추는 것

## 파라미터 Parameter (매개변수)

- ```
  A model parameter is a configuration variable that is internal to the model and whose value can be estimated from data.
  
  - They are required by the model when making predictions.
  - They values define the skill of the model on your problem.
  - They are estimated or learned from data.
  - They are often not set manually by the practitioner.
  - They are often saved as part of the learned model.
  ```

- 모델 내부에서 결정되는 변수, 데이터로부터 결정되는 값. 사용자에 의해 조정되지 않는다.

- ex) 선형 회귀의 계수,  정규분포의 평균과 표준편차 등 

## 하이퍼 파라미터 Hyperparameter

- ```
  A model hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data.
  
  - They are often used in processes to help estimate model parameters.
  - They are often specified by the practitioner.
  - They can often be set using heuristics.
  - They are often tuned for a given predictive modeling problem.
  ```

- 모델링할 때 사용자가 직접 세팅해주는 값.

- 정해진 최적의 값이 없다. 즉, 휴리스틱과 경험법칙에 의해 결정되는 경우가 많다. 

- ex) learning rate, SVM의 C, sigma, KNN의 K 등

## 스트라이드 stride

- 입력층의 윈도우를 움직이면서 필터를 적용함으로써 은닉층의 뉴런을 형성할 때, 윈도우(필터)의 이동량을 의미한다.
  - 주로 1로 설정하지만 변경할 수 있다.

## ReLU 함수

- 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수

## 시그모이드 함수 sigmoid fuction

- 계단함수에 비해 완만한 곡선 형태로 비선형이다. 특정 경계를 기준으로 출력이 확 바뀌어버리는 계단함수와는 달리 시그모이드 함수는 완만하게 매끄럽게 변화하는데 이 매끄러움이 신경망 학습에서 중요하며 활성화 함수로 시그모이드 함수를 사용하는 이유이기도 하다.

## 오차 역전파  Error Backpropagation

- 오차를 점점 거슬러 올라가면서 다시 전파하는 것

- 역전파 방법은 결과 값을 통해서 다시 역으로 input 방향으로 오차를 다시 보내며 가중치를 재업데이트 하는 것이다. 물론 결과에 영향을 많이 미친 노드(뉴런)에 더 많은 오차를 돌려줄 것이다.

- Input이 들어오는 방향(순전파)으로 output layer에서 결과 값이 나온다. 결과값은 오차(error)를 가지게 되는데 역전파는 이 오차(error)를 다시 역방향으로 hidden layer와 input layer로 오차를 다시 보내면서 가중치를 계산하면서 output에서 발생했던 오차를 적용시키는 것이다.

  한 번 돌리는 것을 1 epoch 주기라고 하며 epoch를 늘릴 수록 가중치가 계속 업데이트(학습)되면서 점점 오차가 줄어나가는 방법이다. 

## 경사하강법 Gradient Descent

- 오차를 역전파하여 계속 업데이트 하는 이유는 신경망을 통해 더 나은 결과 값을 내기 위해서 weight를 조정하는데 오차가 영향을 주기 때문이다. 이 때 좀 더 효율적으로 오차를 계산하기 위한 방법이다.

- 경사하강법은 너무나 많은 신경망 안의 가중치 조합을 모두 계산하면 시간이 오래걸리기 때문에 효율적으로 이를 하기위해 고안된 방법입이다. 

- 정확한 답을 얻지는 못할 수도 있다. 단계적으로 접근하는 것이기 때문에 만족스러운 정확도에 이를 때까지 계속해서 답을 찾아나가는 방식이다.

- 신경망의 오차를 경사하강법으로 최저 오차를 찾아나가는 방식이며 신경망의 계산 속도를 빠르게 한다.

- 낮은 쪽의 방향을 찾기 위해 오차함수를 현재 위치에서 미분한다.

- 함수의 최저점을 구하기 좋은 방법으로 신경망과 같이 계산해야 하는 양이 많고(선형대수학) 접근하기가 복잡한 수식에서 잘 작동

- 데이터가 불완전해도 유도리 있게 작동

## 정규화
### normalization

### standardization


# 참고

- https://sacko.tistory.com/